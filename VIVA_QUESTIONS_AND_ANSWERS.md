# üéì Complete Viva Questions & Answers Guide
## Emotion-Based Music Recommendation System

---

## üìã TABLE OF CONTENTS
1. Project Overview Questions
2. Dataset Questions
3. Model Architecture Questions
4. Implementation Questions
5. Technical Deep Dive Questions
6. Results & Performance Questions
7. Future Scope Questions
8. Troubleshooting & Challenges

---

## 1Ô∏è‚É£ PROJECT OVERVIEW QUESTIONS

### Q1: What is your project about?
**Answer:**
"My project is an **Emotion-Based Music Recommendation System** that uses Deep Learning and Computer Vision to detect human emotions in real-time and recommend appropriate music based on the detected emotion.

The system has three main components:
1. **Face Detection** - Using Haar Cascade Classifier
2. **Emotion Recognition** - Using Convolutional Neural Network (CNN)
3. **Music Recommendation** - Mapping emotions to suitable music genres and songs"

---

### Q2: What is the motivation behind this project?
**Answer:**
"Music has a profound impact on human emotions and mental well-being. The motivation behind this project is:
- **Personalization**: Automatically suggest music that matches the user's current emotional state
- **Mental Health**: Help improve mood through appropriate music therapy
- **User Experience**: Eliminate manual searching for mood-appropriate music
- **AI Application**: Demonstrate practical application of Deep Learning in everyday life"

---

### Q3: What are the main features of your system?
**Answer:**
"The system has the following key features:
1. **Real-time Emotion Detection** - Live webcam feed analysis
2. **Image Upload Mode** - Analyze emotions from static images
3. **7 Emotion Categories** - Happy, Sad, Angry, Fear, Surprise, Neutral, Disgust
4. **Music Recommendations** - Genre, playlist, and song suggestions
5. **Interactive Web Interface** - Built with Streamlit
6. **High Accuracy** - 65-70% accuracy on FER-2013 dataset"

---

## 2Ô∏è‚É£ DATASET QUESTIONS

### Q4: Which dataset did you use and why?
**Answer:**
"I used the **FER-2013 (Facial Expression Recognition 2013)** dataset.

**Why FER-2013?**
- **Industry Standard**: Widely accepted benchmark dataset in emotion recognition research
- **Sufficient Size**: Contains 35,887 grayscale images (28,709 training + 7,178 testing)
- **Balanced Classes**: 7 emotion categories with reasonable distribution
- **Standardized**: 48x48 pixel images, pre-processed and labeled
- **Research Backing**: Used in numerous academic papers and competitions
- **Publicly Available**: Free to download from Kaggle"

---

### Q5: What are the 7 emotion categories in your dataset?
**Answer:**
"The FER-2013 dataset contains 7 emotion categories:
1. **Happy** (üòä) - Smiling, joyful expressions
2. **Sad** (üò¢) - Downcast, melancholic expressions
3. **Angry** (üò†) - Frowning, aggressive expressions
4. **Fear** (üò®) - Scared, anxious expressions
5. **Surprise** (üò≤) - Shocked, astonished expressions
6. **Neutral** (üòê) - No particular emotion
7. **Disgust** (ü§¢) - Repulsed, disgusted expressions"

---

### Q6: How is the dataset structured?
**Answer:**
"The dataset is organized in two ways:

**1. Image Folder Structure** (What I used):
```
data/
‚îú‚îÄ‚îÄ train/
‚îÇ   ‚îú‚îÄ‚îÄ angry/
‚îÇ   ‚îú‚îÄ‚îÄ disgust/
‚îÇ   ‚îú‚îÄ‚îÄ fear/
‚îÇ   ‚îú‚îÄ‚îÄ happy/
‚îÇ   ‚îú‚îÄ‚îÄ sad/
‚îÇ   ‚îú‚îÄ‚îÄ surprise/
‚îÇ   ‚îî‚îÄ‚îÄ neutral/
‚îî‚îÄ‚îÄ test/
    ‚îú‚îÄ‚îÄ angry/
    ‚îú‚îÄ‚îÄ disgust/
    ‚îî‚îÄ‚îÄ ... (same structure)
```

**2. CSV Format** (Alternative):
- Single CSV file with pixel values and emotion labels
- Each row: 2304 pixel values (48x48) + emotion label (0-6)

My code supports both formats for flexibility."

---

### Q7: How did you preprocess the dataset?
**Answer:**
"I performed the following preprocessing steps:

**1. Image Loading**:
- Read images using OpenCV (`cv2.imread`)
- Convert to grayscale (already grayscale in FER-2013)

**2. Resizing**:
- Ensure all images are 48x48 pixels
- Use `cv2.resize()` for consistency

**3. Normalization**:
- Scale pixel values from [0, 255] to [0, 1]
- Formula: `pixel_value / 255.0`
- Helps neural network converge faster

**4. Reshaping**:
- Convert to 4D tensor: `(samples, 48, 48, 1)`
- Last dimension is channel (1 for grayscale)

**5. Label Encoding**:
- Convert emotion labels to one-hot encoding
- Example: 'Happy' ‚Üí [0, 0, 0, 1, 0, 0, 0]
- Using `to_categorical()` from Keras"

---

### Q8: What is the train-test split ratio?
**Answer:**
"The FER-2013 dataset comes pre-split:
- **Training Set**: 28,709 images (~80%)
- **Test Set**: 7,178 images (~20%)

Additionally, in my training script, I further split the training data:
- **Training**: 80% of training set
- **Validation**: 20% of training set (for monitoring overfitting)

This gives us three sets:
- Train: ~23,000 images
- Validation: ~5,700 images  
- Test: ~7,178 images"

---

## 3Ô∏è‚É£ MODEL ARCHITECTURE QUESTIONS

### Q9: What model architecture did you use?
**Answer:**
"I used a **Convolutional Neural Network (CNN)** with the following architecture:

**Architecture Overview**:
```
Input (48x48x1)
    ‚Üì
Conv Block 1 (32 filters)
    ‚Üì
Conv Block 2 (64 filters)
    ‚Üì
Conv Block 3 (128 filters)
    ‚Üì
Flatten
    ‚Üì
Dense Layer (256 neurons)
    ‚Üì
Output Layer (7 neurons - softmax)
```

**Each Conv Block contains**:
- Convolutional Layer (3x3 kernel)
- Batch Normalization
- ReLU Activation
- MaxPooling (2x2)
- Dropout (25%)

**Dense Layers**:
- Dense(256) with ReLU
- Batch Normalization
- Dropout (50%)
- Dense(7) with Softmax"

---

### Q10: Why did you choose CNN over other algorithms?
**Answer:**
"I chose CNN because:

**1. Spatial Feature Learning**:
- CNNs automatically learn spatial hierarchies of features
- Detect edges ‚Üí shapes ‚Üí facial features ‚Üí emotions
- No manual feature engineering required

**2. Translation Invariance**:
- Can detect faces regardless of position in image
- Pooling layers provide spatial invariance

**3. Parameter Sharing**:
- Same filters applied across entire image
- Reduces parameters compared to fully connected networks
- Prevents overfitting

**4. Proven Performance**:
- State-of-the-art for image classification tasks
- Industry standard for facial recognition
- Better than traditional ML (SVM, Random Forest) for images

**5. Feature Hierarchy**:
- Early layers: Low-level features (edges, textures)
- Middle layers: Mid-level features (facial parts)
- Deep layers: High-level features (emotion patterns)"

---

### Q11: Explain each layer in your CNN model

**Answer:**
"Let me explain each layer:

**1. Convolutional Layers (Conv2D)**:
- Apply filters/kernels to extract features
- First layer: 32 filters (3x3) - detects basic edges
- Second layer: 64 filters - detects facial parts
- Third layer: 128 filters - detects complex patterns
- Uses 'same' padding to maintain dimensions

**2. Batch Normalization**:
- Normalizes activations between layers
- Reduces internal covariate shift
- Speeds up training and improves stability
- Acts as regularization

**3. Activation (ReLU)**:
- Introduces non-linearity: f(x) = max(0, x)
- Prevents vanishing gradient problem
- Computationally efficient
- Helps model learn complex patterns

**4. MaxPooling2D**:
- Reduces spatial dimensions (2x2 pooling)
- Provides translation invariance
- Reduces computational cost
- Prevents overfitting

**5. Dropout**:
- Randomly drops 25% neurons in conv layers
- Drops 50% neurons in dense layer
- Prevents overfitting
- Forces network to learn robust features

**6. Flatten**:
- Converts 3D feature maps to 1D vector
- Prepares data for dense layers

**7. Dense Layers**:
- Fully connected layers
- 256 neurons for feature combination
- 7 neurons for final classification

**8. Softmax Activation**:
- Converts outputs to probabilities
- Sum of all outputs = 1
- Used for multi-class classification"

---

### Q12: What is the total number of parameters in your model?

**Answer:**
"The model has approximately **1.5 million trainable parameters**.

**Breakdown**:
- Conv Block 1: ~300 parameters
- Conv Block 2: ~18,500 parameters
- Conv Block 3: ~73,800 parameters
- Dense Layer 1: ~3.2 million parameters
- Output Layer: ~1,800 parameters

You can see this by running:
```python
model.summary()
```

The majority of parameters are in the first dense layer because it connects the flattened feature maps (6x6x128 = 4,608 features) to 256 neurons."

---

### Q13: What optimizer and loss function did you use?

**Answer:**
"**Optimizer: Adam (Adaptive Moment Estimation)**
- Learning rate: 0.001 (for minimal model) / 0.0001 (for full model)
- Combines benefits of AdaGrad and RMSprop
- Adaptive learning rates for each parameter
- Works well with sparse gradients
- Requires less hyperparameter tuning

**Loss Function: Categorical Crossentropy**
- Used for multi-class classification (7 emotions)
- Measures difference between predicted and actual probability distributions
- Formula: -Œ£(y_true * log(y_pred))
- Works with softmax activation

**Metrics: Accuracy**
- Percentage of correctly classified emotions
- Easy to interpret and understand"

---

### Q14: What are hyperparameters you used for training?

**Answer:**
"**Training Hyperparameters**:

**For Minimal Model** (quick demo):
- Epochs: 20
- Batch Size: 32
- Learning Rate: 0.001
- Optimizer: Adam
- Validation Split: 20%

**For Full Model** (production):
- Epochs: 50
- Batch Size: 64
- Learning Rate: 0.0001
- Optimizer: Adam
- Validation Split: 20%

**Callbacks**:
1. **EarlyStopping**:
   - Monitors validation loss
   - Patience: 10 epochs
   - Restores best weights
   - Prevents overfitting

2. **ReduceLROnPlateau**:
   - Reduces learning rate when validation loss plateaus
   - Factor: 0.5 (halves the learning rate)
   - Patience: 5 epochs
   - Helps fine-tune the model

3. **ModelCheckpoint**:
   - Saves best model based on validation accuracy
   - Saves to 'model/emotion_model.h5'"

---

## 4Ô∏è‚É£ IMPLEMENTATION QUESTIONS

### Q15: What programming language and libraries did you use?

**Answer:**
"**Programming Language**: Python 3.13

**Deep Learning Framework**:
- TensorFlow 2.20.0
- Keras 3.13.0 (high-level API)

**Computer Vision**:
- OpenCV (cv2) - Face detection, image processing

**Web Framework**:
- Streamlit - Interactive web application

**Data Processing**:
- NumPy - Numerical computations
- Pandas - Data manipulation
- Pillow (PIL) - Image handling

**Visualization**:
- Matplotlib - Plotting training curves

**Optional**:
- Scikit-learn - Data splitting, metrics
- Spotipy - Spotify API integration (future scope)"

---

### Q16: How does face detection work in your system?

**Answer:**
"I use **Haar Cascade Classifier** for face detection:

**How it works**:
1. **Haar Features**:
   - Rectangular features that detect edges, lines, and patterns
   - Calculated using integral images for speed

2. **Cascade of Classifiers**:
   - Multiple stages of weak classifiers
   - Each stage filters out non-face regions
   - Only promising regions pass to next stage

3. **Implementation**:
```python
face_cascade = cv2.CascadeClassifier(
    cv2.data.haarcascades + 'haarcascade_frontalface_default.xml'
)
faces = face_cascade.detectMultiScale(
    gray_image,
    scaleFactor=1.1,
    minNeighbors=5,
    minSize=(30, 30)
)
```

**Parameters**:
- `scaleFactor=1.1`: Image pyramid scaling
- `minNeighbors=5`: Minimum neighbors for detection
- `minSize=(30,30)`: Minimum face size

**Why Haar Cascade?**:
- Fast and efficient (real-time capable)
- Pre-trained and readily available
- Works well for frontal faces
- Low computational requirements"

---

### Q17: Walk me through the complete workflow of your system

**Answer:**
"**Complete Workflow**:

**Step 1: Capture Input**
- Webcam captures video frame OR user uploads image
- Frame is in BGR color format

**Step 2: Face Detection**
- Convert frame to grayscale
- Apply Haar Cascade to detect face(s)
- Extract face region (ROI - Region of Interest)

**Step 3: Preprocessing**
- Resize face to 48x48 pixels
- Normalize pixel values (0-1 range)
- Reshape to (1, 48, 48, 1) for model input

**Step 4: Emotion Prediction**
- Feed preprocessed face to CNN model
- Model outputs 7 probabilities (one per emotion)
- Select emotion with highest probability
- Get confidence score

**Step 5: Music Recommendation**
- Map detected emotion to music database
- Retrieve genres, playlists, and songs
- Shuffle for variety

**Step 6: Display Results**
- Show annotated frame with bounding box
- Display detected emotion with confidence
- Show music recommendations
- Update in real-time (webcam mode)"

---

### Q18: How did you create the training script?

**Answer:**
"My training script (`train_emotion_model.py`) has these components:

**1. Data Loading**:
```python
def load_fer2013_from_images(data_dir="data"):
    # Loads images from train/ and test/ folders
    # Supports both folder structure and CSV format
```

**2. Data Preprocessing**:
- Resize images to 48x48
- Normalize to [0, 1]
- One-hot encode labels
- Split into train/validation

**3. Model Building**:
```python
def build_emotion_model():
    # Creates CNN architecture
    # 3 conv blocks + 2 dense layers
```

**4. Training**:
```python
history = model.fit(
    X_train, y_train,
    validation_data=(X_val, y_val),
    epochs=50,
    batch_size=64,
    callbacks=[early_stop, reduce_lr, checkpoint]
)
```

**5. Evaluation**:
- Test on held-out test set
- Generate confusion matrix
- Plot training curves
- Save model to .h5 file

**6. Visualization**:
- Plot accuracy and loss curves
- Show training vs validation performance"

---

### Q19: Explain your Streamlit web application structure

**Answer:**
"**App Structure** (`app.py`):

**1. Configuration**:
- Page setup (title, icon, layout)
- Custom CSS for styling
- Model loading with caching

**2. Main Function**:
- Header and title
- Sidebar with mode selection
- Model status display
- Footer

**3. Three Modes**:

**a) Webcam Mode**:
- Real-time video capture
- Continuous emotion detection
- Live music recommendations
- Start/stop camera control

**b) Upload Mode**:
- File uploader for images
- Single image analysis
- Display original and annotated image
- Show emotion and recommendations

**c) About Page**:
- Project information
- How it works
- Supported emotions
- Technical details

**4. Helper Functions**:
- `load_models()`: Cached model loading
- `display_recommendations()`: Format music suggestions
- `webcam_mode()`: Handle real-time detection
- `upload_mode()`: Handle image upload

**5. Styling**:
- Custom CSS with gradients
- Animated emotion boxes
- Hover effects
- Responsive design"

---

### Q20: How does the music recommendation system work?

**Answer:**
"**Music Recommendation Logic** (`music_recommender.py`):

**1. Emotion-Music Mapping**:
- Dictionary mapping each emotion to music attributes
- Each emotion has: genres, playlists, songs

**2. Mapping Examples**:
```python
"Happy" ‚Üí {
    "genres": ["Pop", "Dance", "Upbeat"],
    "playlists": ["Happy Hits", "Feel Good Pop"],
    "songs": ["Happy - Pharrell Williams", ...]
}

"Sad" ‚Üí {
    "genres": ["Acoustic", "Piano", "Blues"],
    "playlists": ["Sad Songs", "Melancholy Piano"],
    "songs": ["Someone Like You - Adele", ...]
}
```

**3. Recommendation Process**:
- Input: Detected emotion (e.g., "Happy")
- Lookup emotion in mapping dictionary
- Retrieve genres, playlists, songs
- Shuffle songs for variety
- Return top 3 playlists and 5 songs

**4. Why This Approach?**:
- **Simple**: Easy to understand and maintain
- **Fast**: Instant recommendations (no ML needed)
- **Customizable**: Easy to add new songs/genres
- **Reliable**: Deterministic results
- **Scalable**: Can integrate with Spotify API later"

---

## 5Ô∏è‚É£ TECHNICAL DEEP DIVE QUESTIONS

### Q21: What is a Convolutional Layer and how does it work?

**Answer:**
"**Convolutional Layer** applies filters to extract features:

**How it works**:
1. **Filter/Kernel**: Small matrix (e.g., 3x3) with learnable weights
2. **Convolution Operation**:
   - Slide filter across input image
   - Element-wise multiplication
   - Sum results to get one output value
3. **Multiple Filters**: Each filter detects different features
4. **Feature Maps**: Output of each filter

**Example**:
- Input: 48x48x1 image
- Filter: 3x3x1 kernel
- 32 filters ‚Üí 32 feature maps
- Output: 48x48x32 (with 'same' padding)

**What it learns**:
- First layer: Edges, gradients
- Second layer: Textures, patterns
- Third layer: Complex shapes, facial features

**Advantages**:
- Parameter sharing (same filter for entire image)
- Spatial hierarchy learning
- Translation invariance"

---

### Q22: What is Batch Normalization and why use it?

**Answer:**
"**Batch Normalization** normalizes layer inputs:

**What it does**:
1. For each mini-batch:
   - Calculate mean (Œº) and variance (œÉ¬≤)
   - Normalize: x_norm = (x - Œº) / ‚àö(œÉ¬≤ + Œµ)
   - Scale and shift: y = Œ≥ * x_norm + Œ≤
   - Œ≥ and Œ≤ are learnable parameters

**Benefits**:
1. **Faster Training**:
   - Allows higher learning rates
   - Reduces training time by 2-3x

2. **Reduces Internal Covariate Shift**:
   - Stabilizes distribution of layer inputs
   - Each layer doesn't have to adapt to changing inputs

3. **Regularization Effect**:
   - Adds noise (batch statistics)
   - Reduces need for dropout
   - Helps prevent overfitting

4. **Better Gradient Flow**:
   - Prevents vanishing/exploding gradients
   - Improves backpropagation

**In my model**:
- Applied after each Conv2D layer
- Applied after Dense layer
- Improves model stability and performance"

---

### Q23: What is Dropout and how does it prevent overfitting?

**Answer:**
"**Dropout** randomly deactivates neurons during training:

**How it works**:
1. During training:
   - Randomly set neurons to 0 with probability p
   - In my model: p=0.25 (conv layers), p=0.5 (dense layer)
   - Different neurons dropped each iteration

2. During testing:
   - All neurons active
   - Outputs scaled by (1-p)

**Why it prevents overfitting**:
1. **Prevents Co-adaptation**:
   - Neurons can't rely on specific other neurons
   - Forces learning of robust features

2. **Ensemble Effect**:
   - Each iteration trains different sub-network
   - Final model is like ensemble of many networks

3. **Reduces Complex Co-adaptations**:
   - Can't memorize training data
   - Must learn generalizable patterns

**Analogy**:
Like studying with different groups of friends each time - you learn concepts more deeply rather than relying on specific people.

**In my model**:
- 25% dropout after each conv block
- 50% dropout before output layer
- Significantly improves test accuracy"

---

### Q24: What is the difference between training and validation accuracy?

**Answer:**
"**Training Accuracy**:
- Performance on data the model has seen during training
- Model learns from this data
- Usually higher than validation accuracy

**Validation Accuracy**:
- Performance on unseen data (held-out from training)
- Model doesn't learn from this data
- Better indicator of real-world performance

**Why we need both**:

**1. Detect Overfitting**:
- Training accuracy ‚Üë, Validation accuracy ‚Üì ‚Üí Overfitting
- Model memorizing training data, not generalizing

**2. Detect Underfitting**:
- Both accuracies low ‚Üí Underfitting
- Model too simple or not trained enough

**3. Ideal Scenario**:
- Both accuracies high and close
- Small gap is normal (2-5%)
- Indicates good generalization

**In my project**:
- Monitor both during training
- Use validation accuracy for early stopping
- Save model with best validation accuracy
- Final evaluation on separate test set"

---

### Q25: Explain the concept of epochs and batch size

**Answer:**
"**Epoch**:
- One complete pass through entire training dataset
- If dataset has 28,000 images, 1 epoch = model sees all 28,000 images once
- My model: 20 epochs (minimal) or 50 epochs (full)

**Batch Size**:
- Number of samples processed before updating weights
- My model: 32 (minimal) or 64 (full)

**How they work together**:
- Dataset: 28,000 images
- Batch size: 64
- Batches per epoch: 28,000 / 64 = 437 batches
- 50 epochs = 50 √ó 437 = 21,850 weight updates

**Batch Size Trade-offs**:

**Small Batch (e.g., 32)**:
- ‚úÖ More frequent updates
- ‚úÖ Better generalization
- ‚úÖ Less memory usage
- ‚ùå Slower training
- ‚ùå Noisy gradients

**Large Batch (e.g., 128)**:
- ‚úÖ Faster training
- ‚úÖ Stable gradients
- ‚úÖ Better GPU utilization
- ‚ùå More memory needed
- ‚ùå May overfit

**Why I chose 64**:
- Good balance between speed and accuracy
- Fits in GPU memory
- Stable gradient estimates
- Industry standard"

---

### Q26: What is the Softmax activation function?

**Answer:**
"**Softmax** converts raw scores to probabilities:

**Formula**:
```
softmax(x_i) = e^(x_i) / Œ£(e^(x_j))
```

**Example**:
Raw outputs: [2.0, 1.0, 0.1, 3.0, 0.5, 0.2, 0.3]

After softmax: [0.12, 0.04, 0.02, 0.32, 0.03, 0.02, 0.02]

**Properties**:
1. All outputs between 0 and 1
2. Sum of all outputs = 1.0
3. Can be interpreted as probabilities
4. Emphasizes largest value

**In my model**:
- Applied to final layer (7 neurons)
- Each neuron represents one emotion
- Output: probability distribution over 7 emotions
- Example: [Happy: 0.65, Sad: 0.15, Angry: 0.10, ...]
- Prediction: argmax (emotion with highest probability)

**Why Softmax for multi-class?**:
- Mutually exclusive classes (one emotion at a time)
- Probabilistic interpretation
- Works well with categorical crossentropy loss
- Differentiable (good for backpropagation)"

---

### Q27: How does backpropagation work in your CNN?

**Answer:**
"**Backpropagation** updates weights to minimize loss:

**Forward Pass**:
1. Input image ‚Üí Conv layers ‚Üí Dense layers ‚Üí Output
2. Calculate predictions
3. Compute loss (difference from true label)

**Backward Pass**:
1. Calculate gradient of loss w.r.t. output
2. Propagate gradient backward through layers
3. Use chain rule to compute gradients for each weight
4. Update weights using optimizer (Adam)

**Gradient Flow in CNN**:
```
Loss ‚Üí Softmax ‚Üí Dense ‚Üí Flatten ‚Üí Conv3 ‚Üí Conv2 ‚Üí Conv1
```

**Weight Update**:
```
weight_new = weight_old - learning_rate √ó gradient
```

**Adam Optimizer Enhancement**:
- Maintains moving averages of gradients
- Adaptive learning rates per parameter
- Momentum for faster convergence

**Challenges**:
- **Vanishing Gradients**: Solved by ReLU, Batch Normalization
- **Exploding Gradients**: Solved by gradient clipping, normalization

**In my model**:
- Automatic backpropagation via TensorFlow
- Adam optimizer handles weight updates
- Batch normalization helps gradient flow
- Dropout prevents overfitting during training"

---

### Q28: What is overfitting and how did you prevent it?

**Answer:**
"**Overfitting**: Model performs well on training data but poorly on new data

**Signs of Overfitting**:
- Training accuracy: 95%
- Validation accuracy: 60%
- Large gap indicates memorization, not learning

**How I prevented overfitting**:

**1. Dropout (25% and 50%)**:
- Randomly deactivates neurons
- Prevents co-adaptation
- Forces robust feature learning

**2. Batch Normalization**:
- Regularization effect
- Adds noise to training
- Improves generalization

**3. Data Augmentation** (can be added):
- Rotate, flip, zoom images
- Increases effective dataset size
- Model sees variations

**4. Early Stopping**:
- Monitors validation loss
- Stops when validation stops improving
- Prevents training too long

**5. Regularization**:
- L2 regularization (can be added)
- Penalizes large weights
- Simpler model

**6. Sufficient Data**:
- 28,000+ training images
- More data = better generalization

**7. Model Complexity**:
- Not too deep (3 conv blocks)
- Balanced capacity
- Appropriate for dataset size"

---


